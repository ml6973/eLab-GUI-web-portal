{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />\n",
    "<img style=\" float:right; display:inline\" src=\"http://opencloud.utsa.edu/wp-content/themes/utsa-oci/images/logo.png\"/>\n",
    "\n",
    "### **University of Texas at San Antonio** \n",
    "<br/>\n",
    "<br/>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 2.5em;\"> **Open Cloud Institute** </span>\n",
    "\n",
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Parallel and Distributed Computing Course\n",
    "\n",
    "<br/>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Paul Rad, Ph.D.** </span>  \n",
    "\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Swanand Mhalagi, Research Assistant** </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.4em;\"> *Open Cloud Institute, University of Texas at San Antonio, San Antonio, Texas, USA* </span>  \n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.4em;\"> {Swanand.Mhalagi, Paul.Rad}@utsa.edu </span>  \n",
    "\n",
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Introduction:** </span> <span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Over the last years, high performance computing has become an affordable resource to many more researchers in the scientific community than ever before. The conjunction of quality open source software and commodity hardware strongly influenced the now widespread popularity of cluster computing. </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Among many parallel computational models, message-passing has proven to be an effective one. This paradigm is specially suited for distributed memory architectures and is used in today’s most demanding scientific and engineering application related to modeling, simulation, design, and signal processing. However, portable message-passing parallel programming used to be a nightmare in the past because of the many incompatible options developers were faced to. Fortunately, this situation definitely changed after the MPI Forum released its standard specification.</span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Basics of MPI:** </span>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> MPI, the Message Passing Interface, is a standardized and portable message-passing system designed to function on a wide variety of parallel computers. The standard defines the syntax and semantics of library routines and allows users to write portable programs in the main scientific programming languages (Fortran, C, or C++). MPI provides a very powerful and very general way of expressing parallelism. MPI makes it possible for programmers to develop portable, parallel software libraries. It is worth reemphasizing, one of the most important contributions of MPI is its support for parallel libraries.  </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Your 1st python MPI program:** </span>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> As tradition has it, we will introduce you to MPI programming using a variation on the standard hello world program: your first MPI python program will be the Hello World program for multiple processes. The source code is as follows: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<'EOF'> hello.py\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "print \"hello world from process \", rank\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> To run the above program we need a special instruction mentioned below. The -n 5 option specifies the desired number of processes. In our case, 5 processes are run, each one being an instance of python. To each of the 5 instances of python, we pass the argument hello.py which is the name of our program’s text file, located in the current directory. </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.4em;\"> *mpiexec  -n 5 python  hello.py * </span> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Above command will throw a warning since these machines dont have InfiniBand(IB) connection. In order to over come this warning use below mentioned command</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world from process  3\r\n",
      "hello world from process  0\r\n",
      "hello world from process  4\r\n",
      "hello world from process  1\r\n",
      "hello world from process  2\r\n"
     ]
    }
   ],
   "source": [
    "!mpiexec  -n 5 -mca btl ^openib python  hello.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> The above command will execute five python processes which can all communicate with each other. When each program runs, it will print hello, and tell you its rank. \n",
    "Notice that when you try this multiple times, they do not necessarily print in order. This is because 5 separate processes are running on different processors, and we cannot know beforehand which one will execute its print statement first. If the processes are being scheduled on the same processor instead of multiple processors, then it is up to the operating system to schedule the processes, and it has no preference of any one of our processes over any other process of ours. In essence, each process executes autonomously. Each of the five instances of python then opens the .py file and runs the same program. The difference in each process’s execution environment is that the processes are given different ranks in the communicator. Because of this, each process prints a different number when it executes. </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
